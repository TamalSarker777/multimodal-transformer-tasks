{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üß† Visual Question Answering with ViLT on MEDPIX-ClinQA\n\nThis notebook demonstrates a Visual Question Answering (VQA) pipeline using the [MEDPIX-ClinQA](https://huggingface.co/datasets/adishourya/MEDPIX-ClinQA) dataset, where the model learns to answer clinically relevant questions based on medical images.\n\nWe fine-tune the [`dandelin/vilt-b32-mlm`](https://huggingface.co/dandelin/vilt-b32-mlm) vision-language model on this task, treating VQA as a **multi-label classification** problem. Each answer is mapped to an integer label, and label weights are used to reflect answer frequencies where applicable.\n\n**üîç Key Information:**\n- **Model:** `dandelin/vilt-b32-mlm` (ViLT - Vision-and-Language Transformer)\n- **Task:** Visual Question Answering (VQA)\n- **Dataset:** `adishourya/MEDPIX-ClinQA` from Hugging Face ü§ó\n- **Preprocessing:** Image normalization, question tokenization, label ID mapping\n- **Objective:** Predict medically accurate answers from paired image-question inputs\n\nThis project serves as a practical application of ViLT for multimodal clinical reasoning and can be adapted for other VQA datasets or tasks.\n","metadata":{}},{"cell_type":"code","source":"# Initialize huggingface env\nfrom huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Import dataset and preprocess","metadata":{}},{"cell_type":"code","source":"# load dataset\nfrom datasets import load_dataset\n\nds = load_dataset(\"adishourya/MEDPIX-ClinQA\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Take a subset of 5000 examples\nds = ds[\"train\"].select(range(5000))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# clear unnecessary columns\ndataset = ds.remove_columns(['mode', 'case_id',])\ndataset[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize an image\nfrom IPython.display import display\n\nimage = dataset[0]['image_id']\ndisplay(image)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Collect all unique answers\nall_answers = set(example[\"answer\"] for example in dataset)\n\n# Step 2: Create label mappings\nlabel2id = {label: idx for idx, label in enumerate(sorted(all_answers))}\nid2label = {idx: label for label, idx in label2id.items()}\n\n# # if want short answer\n# def summarize_answer(answer):\n#     return answer.split('.')[0].strip()  # take first sentence or phrase\n\n# short_labels = set(summarize_answer(example[\"answer\"]) for example in dataset)\n# label2id = {label: idx for idx, label in enumerate(sorted(short_labels))}\n# id2label = {idx: label for label, idx in label2id.items()}\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# id2label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# replacing all the answer to ids for training purpose\ndef replace_answers_with_ids(example):\n    example[\"label\"] = label2id[example[\"answer\"]]\n    return example\n\ndataset = dataset.map(replace_answers_with_ids)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset.features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Import Vilt processor\n","metadata":{}},{"cell_type":"code","source":"# importing Vilt processor\nfrom transformers import ViltProcessor\n\nmodel_name = \"dandelin/vilt-b32-mlm\"\nprocessor = ViltProcessor.from_pretrained(model_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To preprocess the data we need to encode the images and questions using the ViltProcessor. The processor will use the BertTokenizerFast to tokenize the text and create input_ids, attention_mask and token_type_ids for the text data. As for images, the processor will leverage ViltImageProcessor to resize and normalize the image, and create pixel_values and pixel_mask.","metadata":{}},{"cell_type":"code","source":"import torch\n\n\ndef preprocess_data(batch):\n    # Convert each image to RGB\n    images = [img.convert(\"RGB\") for img in batch['image_id']]\n    texts = batch['question']\n\n    # Tokenize with the processor\n    encoding = processor(images, texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n\n    # Remove batch dimension manually\n    for k, v in encoding.items():\n        encoding[k] = v\n\n    # Create soft labels\n    targets = []\n    for label in batch['label']:\n        target = torch.zeros(len(id2label))\n        target[label] = 1.0\n        targets.append(target)\n\n    encoding[\"labels\"] = targets\n    return encoding\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# mapping the preprocess with the dataset\nprocessed_dataset = dataset.map(\n    preprocess_data,\n    batched=True,\n    batch_size=5,\n    remove_columns=['image_id', 'question', 'answer']\n)\n\nprocessed_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Adding a datacollator","metadata":{}},{"cell_type":"code","source":"from transformers import DefaultDataCollator\n\ndata_collator = DefaultDataCollator()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Import Model","metadata":{}},{"cell_type":"code","source":"from transformers import ViltForQuestionAnswering\n\nmodel = ViltForQuestionAnswering.from_pretrained(model_name, num_labels=len(id2label), id2label=id2label, label2id=label2id)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Importing Training Arguments","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nrepo_id = \"Tamal/vilt_finetuned_5000\"\n\ntraining_args = TrainingArguments(\n    output_dir=repo_id,\n    per_device_train_batch_size=5,\n    num_train_epochs=20,\n    save_steps=200,\n    logging_steps=50,\n    learning_rate=5e-5,\n    save_total_limit=2,\n    remove_unused_columns=False,\n    push_to_hub=True,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=processed_dataset,\n    processing_class=processor,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"processor = ViltProcessor.from_pretrained(\"Tamal/vilt_finetuned_5000\")\n\nimage = Image.open(example['image_id'])\nquestion = example['question']\n\n# prepare inputs\ninputs = processor(image, question, return_tensors=\"pt\")\n\nmodel = ViltForQuestionAnswering.from_pretrained(\"Tamal/vilt_finetuned_5000\")\n\n# forward pass\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nlogits = outputs.logits\nidx = logits.argmax(-1).item()\nprint(\"Predicted answer:\", model.config.id2label[idx])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Zero shot answering question\nEarlier models treated Visual Question Answering (VQA) as a classification problem, while newer models like BLIP-2 approach it as a generative task using vision-language pretraining. BLIP-2 allows combining any vision encoder with an LLM, achieving state-of-the-art results on tasks like VQA.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoProcessor, Blip2ForConditionalGeneration\nimport torch\nfrom accelerate.test_utils.testing import get_backend\n\nprocessor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\ndevice, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\n\nimage = example['image_id'] \nquestion = example['question']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = f\"Question: {question} Answer:\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=10)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\nprint(generated_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}